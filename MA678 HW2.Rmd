---
title: "MA678 Homework 2"
date: "9/26/2024"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(rstanarm)
library(dplyr)
```

## 11.5 
*Residuals and predictions*: The folder `Pyth` contains outcome $y$ and predictors $x_1$, $x_2$ for 40 data points, with a further 20 points with the predictors but no observed outcome. Save the file to your working directory, then read it into R using `read.table()`.

```{r}
pythData <- "https://raw.githubusercontent.com/avehtari/ROS-Examples/refs/heads/master/"
Pyth <- read.table(paste0(pythData, "Pyth/pyth.txt"), header = TRUE)
#pulling out data from dataset
```

### (a) 
Use R to fit a linear regression model predicting $y$ from $x_1$, $x_2$, using the first 40 data points in the file. Summarize the inferences and check the fit of your model.

```{r}
LM = lm(y ~ x1 + x2, data = Pyth, subset = 1:40)
stanGlm = stan_glm(y ~ x1 + x2, data = Pyth, subset = 1:40, refresh = 0)
summary(stanGlm) #summarize the inferences
pp_check(stanGlm) #check the fit of your model
```

### (b) 
Display the estimated model graphically as in Figure 10.2

```{r}
par(mfrow = c(1, 2)) #combines graphs into one screen
plot(Pyth[1:40, ]$x1, Pyth[1:40, ]$y, 
     xlab = "x1", 
     ylab = "y") #plots and labeled axis
curve(coef(LM)[1] + coef(LM)[2] * x + coef(LM)[3] * mean(Pyth[1:40, ]$x2),
      add = TRUE) #regression line
plot(Pyth[1:40, ]$x2, Pyth[1:40, ]$y, 
     xlab = "x2", 
     ylab = "y") #plots and labeled axis
curve(coef(LM)[1] + coef(LM)[2] * mean(Pyth[1:40, ]$x1) + coef(LM)[3] * x,
      add = TRUE) #regression line
```

### (c) 
Make a residual plot for this model. Do the assumptions appear to be met?

```{r}
predicted <- predict(stanGlm) #predicted stanGlm
residual <- Pyth$y[1:40] - predicted #residual equation
par(mar = c(3,3,2,1), mgp = c(2, 1, 0)) #gives area to label axis
plot(predicted, residual,
     xlab = "Predicted", 
     ylab = "Residuals", 
     main = "Residuals vs Predicted") #plot and labeled axis
abline(h = 0) #regression line at y = 0 to see if error assumptions are met

cat("From the scatterplot, the assumption appears to not be met")
```


### (d) 
Make predictions for the remaining 20 data points in the file. How confident do you feel about these predictions?

```{r}
p = posterior_predict(stanGlm, newdata = Pyth[41:60, 2:3])
# looking at remaining values with newdata
p_upper = apply(p, MARGIN = 2, FUN = quantile, probs = 0.95)
# 95% probability 
p_lower = apply(p, MARGIN = 2, FUN = quantile, probs = 0.05)
# 1 - 95% probability
p_mean = apply(p, MARGIN = 2, FUN = mean)
# setting mean prediction
pre = data.frame(upper = p_upper, lower = p_lower, 
                 point_estimate = p_mean, 
                 index = 41:60)
# data frame setup
ggplot(pre) + 
  geom_point(aes(index, point_estimate)) + 
  geom_segment(aes(index, xend = index, 
                   upper, yend = lower)) + 
  labs(x = "Index", 
       y = "95% Prediction Intervals",
       title = "Prediction for Final 20 Data Points")
# plot and labeled axis

cat("Based on the 95% prediction intervals. There seems to be high confidence in the predictions.")
```


## 12.5 
*Logarithmic transformation and regression*: Consider the following regression:
$$\log(\text{weight})=-3.8+2.1 \log(\text{height})+\text{error,} $$
with errors that have standard deviation 0.25. Weights are in pounds and heights are in inches.

### (a) 
Fill in the blanks: Approximately 68% of the people will have weights within a factor of 1.284 and 0.25 of their predicted values from the regression.

### (b) 
Using pen and paper, sketch the regression line and scatterplot of log(weight) versus log(height) that make sense and are consistent with the fitted model. Be sure to label the axes of your graph.

```{r}
height = rnorm(n = 1000, mean = 65, sd = 7)
# Looked up average height in a male 171 and female 159 and averaged it expected a 50/50 population. Also standard deviation was a little under 3 inches, thus giving the standard deviation of 7 cm.
weight = exp(-3.8 + 2.1 * log(height) + rnorm(n = 1000, mean = 0, sd = 0.25)) #used given values
globals = data.frame(weight = weight, height = height)
ggplot(globals, aes(x = log(height), y = log(weight))) + 
  geom_point() + 
  geom_smooth(formula = 'y~x', method = "lm") #plot and labeled axis
```

## 12.6 
*Logarithmic transformations*: The folder `Pollution` contains mortality rates and various environmental factors from 60 US metropolitan areas. For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. this model is an extreme oversimplication, as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformation in regression.  

```{r}
Pollution <- read.csv("pollution.csv", header=TRUE)
# setting pollution.csv from git
```

### (a) 
Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
ggplot(data = Pollution) + 
  geom_point(aes(nox, mort)) #graph scatterplot
lm_model = lm(mort ~ nox, Pollution)
summary(lm_model) #summarize the linear model

cat("Based on the regression model. The linear regression will not fit the data well.")

par(mar = c(3,3,2,1), mgp = c(2,1,0))
plot(lm_model, which = 1) #evaluated the residual plot from the regression

```

### (b) 
Find an appropriate reansformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

```{r}
# I used log(nox) vs mort because it seemed from 12.6a that nox had to large of outliers that would fit well if changed into a log model.
ggplot(data = Pollution, aes(x = log(nox), y = mort)) + 
  geom_point() + 
  geom_smooth(formula = "y ~ x", method = "lm") + 
  labs(x = "log(nox)",
       y = "mort", 
       title = "log(nox) vs mort") #plot and labeled axis
lm_model = lm(mort ~ log(nox), data = Pollution)
summary(lm_model) #summarize the logarithmic regression model

par(mar = c(3,3,2,1), mgp = c(2,1,0))
plot(lm_model, which = 1) #evaluated the residual plot from the regression
```

### (c) 
Interpret the slope coefficient from the model you chose in (b)


For every 1 increase in log(nitric oxides), on average there is a 15.335 increase in mortality


### (d) 
Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformation when helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
lm_model = lm(mort ~ log(nox) + log(so2) + log(hc), data = Pollution)
summary(lm_model) #summarize the logarithmic regression model

par(mar = c(3,3,2,1), mgp = c(2, 1, 0))
plot(lm_model, which = 1) #plot the fitted regression model

cat("The coefficients: \n
intercept - when all other values are 1 (since we are dealing with a logarithmic model) our average expected value is 924.965 in mortality \n
log(nox) - for every 1 increase in log(nitric oxides), on average there is a 58.336 increase in mortality \n
log(so2) - for every 1 increase in log(sulfur dioxide), on average there is a 11.762 increase in mortality \n
log(hc) - for every 1 increase in log(hydrocarbons), on average there is a 57.3 decrease in mortality") #written discription
```

### (e) 
Cross validate: fit the model you chose above to the first half of the data and then predict for the second half. You used all the data to construct the model in (d), so this is not really cross validation, but it gives a sense of how the steps of cross validation can be implemented.

```{r}
n = dim(Pollution)[1] / 2 #finding first half of data
lm_model = lm(mort ~ log(nox) + log(so2) + log(hc), data = Pollution, subset = 1:n) #linear regression model of 12.6d
summary(lm_model) #summarize the logarithmic regression model

plm_model = predict(lm_model, 
                    newdata = Pollution[n:(2 * n), ]) #second half data
ggplot() + 
  geom_point(aes(plm_model, plm_model - 
                   Pollution$mort[n:(2 * n)])) + 
  geom_hline(yintercept = 0) + 
  labs(x = "Predicted", 
       y = "Predicted Error",
       title = "Predicted vs Predicted Error") #plot and labeled axis

```

## 12.7 
*Cross validation comparison of models with different transformations of outcomes*: when we compare models with transformed continuous outcomes, we must take into account how the nonlinear transformation warps the continuous outcomes. Follow the procedure used to compare models for the mesquite bushes example on page 202.

### (a) 
Compare models for earnings and for log(earnings) given height and sex as shown in page 84 and 192. Use `earnk` and `log(earnk)` as outcomes.

```{r}
Earnings <- read.csv("earnings.csv")
Earnings$log_earnk <- log(Earnings$earnk)

earnings <- na.omit(Earnings) #getting rid of NA
earnings <- earnings[!is.infinite(earnings$log_earnk),]

regEarnK <- stan_glm(earnk ~ height + male, data = earnings, refresh = 0)
logEarnK <- stan_glm(log_earnk ~ height + male, data = earnings, refresh = 0)
#Reporting these Bayesian regression into models

regEarnK
logEarnK
```

### (b) 
Compare models from other exercises in this chapter.

Comparing the models from other exercises, for the log(earnK), it seems as if it is similar to the other models in a sense that they are very centered and stable due to the very low standard deviation (the normal distribution). Even for the regular earnK model, we can see that the standard deviation is also really low resulting in similar results as it's log counterpart. 

## 12.8 
*Log-log transformations*: Suppose that, for a certain population of animals, we can predict log weight from log height as follows:  

* An animal that is 50 centimeters tall is predicted to weigh 10 kg.

* Every increase of 1% in height corresponds to a predicted increase of 2% in weight.

* The weights of approximately 95% of the animals fall within a factor of 1.1 of predicted values.

### (a) 
Give the equation of the regression line and the residual standard deviation of the regression.

Since log(weight) = 0.02 * log(height) + (log(10) - 2log(50))
The equation of the regression line is:
      log(weight) = 0.02 * log(height) - 5.52 + error

For the residual standard deviation:
      0.0477

### (b) 
Suppose the standard deviation of log weights is 20% in this population. What, then, is the $R^{2}$ of the regression model described here?  

For R^2, we have: 1 - (0.477^2)/(0.2)^2
    = 0.9431

## 12.9 
*Linear and logarithmic transformations*: For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats. Discuss the advantages and disadvantages of the following measures:  

### (a) 
The simple difference, $D_i - R_i$

Advantage: When centered (Di = Ri), the visualization and the output is very easy to interpret.

Disadvantage: When it comes to a single output, negative numbers can be an issue with interpreting the single variable; in addition, even with absolute values of these differences will only confuse which party has the more money raised than the other.

### (b) 
The ratio, $D_i / R_i$

Advantage: There might be times where we need to see the percent difference between the two values and we can see how much more funding percentage one party received compared to the other party.

Disadvantage: When dealing with ratios, when they are centered the value will be at 1 instead of 0 since a number/number = 1 and this can cause confusion due to it being asymmetric.

### (c) 
The difference on the logarithmic scale, $\log D_i - \log R_i$ 

Advantage: This can show the differences in percentage of funding each party received, which can be good to know in certain statistics

Disadvantage: The log graphs are usually easier to graph in certain aspects of exponentially increasing values but harder to interpret when it comes to explaining it's visualization.

### (d) 
The relative proportion, $D_{i}/(D_{i}+R_{i})$.

Advantage: This can show important information to those who wants to see the percentage of funding for a specific party in comparison to the whole, which can give the reader a good description of the bigger picture.

Disadvantage: Just like 12.9b, the center will be in this case at 0.5 instead of 0 and this can cause confusion.
If the values of Di and Ri are both 0, then there will be an error in the equation.


## 12.11
*Elasticity*: An economist runs a regression examining the relations between the average price of cigarettes, $P$, and the quantity purchased, $Q$, across a large sample of counties in the United  States, assuming the functional form, $\log Q=\alpha+\beta \log P$. Suppose the estimate for $\beta$ is 0.3.  Interpret this coefficient. 

Since we are dealing with a log function, for every 1% increase we have in cigarette prices, on average we see a 0.3% increase in the quantity purchased.

## 12.13
*Building regression models*: Return to the teaching evaluations data from Exercise 10.6. Fit regression models predicting evaluations given many of the inputs in the dataset. Consider interactions, combinations of predictors, and transformations, as appropriate. Consider several  models, discuss in detail the final model that you choose, and also explain why you chose it rather than the others you had considered. 

```{r}
Beauty <- read.csv("beauty.csv", header = TRUE)

beauty1 <- stan_glm(eval ~ beauty, data = Beauty, refresh = 0)
Beauty$prediction1 = predict(beauty1, newdata = Beauty)
Beauty$minority <- as.factor(Beauty$minority)
beauty2 <- stan_glm(eval ~ beauty + minority, data = Beauty, refresh = 0)
Beauty$prediction2 = predict(beauty2, newdata = Beauty)
beauty3 <- stan_glm(eval ~ beauty + minority + beauty * minority, data = Beauty, refresh = 0)
Beauty$prediction3 = predict(beauty3, newdata = Beauty)
# Set up all the new columns as predictions1-3

mean_eval <- mean(Beauty$eval)
mean1 <- mean(Beauty$prediction1) - mean_eval
mean2 <- mean(Beauty$prediction2) - mean_eval
mean3 <- mean(Beauty$prediction3) - mean_eval
# Used the predictions to find the difference in means from the actual mean of the evaluation

mean1
mean2
mean3

summary(beauty1)
summary(beauty2)
summary(beauty3)
```


## 12.14
Prediction from a fitted regression: Consider one of the fitted models for mesquite leaves, for example `fit_4`, in Section 12.6. Suppose you wish to use this model to make inferences about the average mesquite yield in a new set of trees whose predictors are in data frame called  new_trees. Give R code to obtain an estimate and standard error for this population average. You do not need to make the prediction; just give the code. 

```{r}
# mesquiteData <- "https://raw.githubusercontent.com/avehtari/ROS-Examples/refs/heads/master/"
# mesquite <- read.table(paste0(mesquiteData, "Mesquite/data/mesquite.dat"), header = TRUE)
#  mesquite$cvolume <- mesquite$diam1 * mesquite$diam2 * mesquite$canopy_height
#  mesquite$carea <- mesquite$diam1 * mesquite$diam2
#  mesquite$cshape <- mesquite$diam1 / mesquite$diam2
#  # The new columns that was created into the mesquite data
# 
#  fit_4 <- stan_glm(formula = log(weight) ~ log(cvolume) +
#                      log(carea) + log(cshape) +
#                      log(total_height) + log(density) + group,
#                    data = mesquite, refresh = 0) #set the fitted models into fit_4
#  p = posterior_predict(fit_4, newdata = new_trees, fun = exp)
#  # Code will NOT run because we do not have the new_trees data frame just making an inference
#  pavg = apply(p, MARGIN = 2, FUN = mean)
#  popavg = mean(pavg) #computed population average
#  popmsd = sd(pmean) #computed standard error
# 
#  # This is the code when the new_trees data.frame would have loaded
```
